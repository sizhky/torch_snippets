{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp s3_loader2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import boto3\n",
    "import os\n",
    "import datetime\n",
    "from datetime import tzinfo\n",
    "from dateutil.tz import tzutc\n",
    "from torch_snippets import stem, fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_snippets.s3_loader2 import S3FileHandler\n",
    "\n",
    "aws_access_key_id = \"AKIAQFXXXXXXXX6CN\"\n",
    "aws_secret_access_key = \"AC3AJsZ6XXXXXXXXXXXXXXXXXejfNN9h\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "# | hide\n",
    "class S3FileHandler:\n",
    "    def __init__(self, aws_access_key, aws_secret_access_key):\n",
    "        self.s3_client = boto3.client('s3', aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key)\n",
    "\n",
    "    def list_s3_buckets(self):\n",
    "        \"\"\"\n",
    "        Lists all the s3 buckets in s3\n",
    "        \"\"\"        \n",
    "        try:\n",
    "            # Call S3 to list current buckets\n",
    "            response = self.s3_client.list_buckets()\n",
    "            buckets = [bucket['Name'] for bucket in response['Buckets']]\n",
    "            return buckets\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    def list_s3_objects(self, bucket_name, key=\"\"):\n",
    "        \"\"\"\n",
    "        List all files in an S3 bucket or within a specific prefix.\n",
    "\n",
    "        :param bucket_name: str. Name of the S3 bucket.\n",
    "        :param key: str or None. Specific prefix to list files from, defaults to None.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Initialize a paginator for listing objects\n",
    "            paginator = self.s3_client.get_paginator('list_objects_v2')\n",
    "            # Use the paginator to fetch all objects in the specified bucket and prefix if provided\n",
    "            files = dict()\n",
    "            for page in paginator.paginate(Bucket=bucket_name, Prefix=key):\n",
    "                # Access the 'Contents' from the page, which lists the objects\n",
    "                if 'Contents' in page:\n",
    "                    for obj in page['Contents']:\n",
    "                        files[obj['Key']] = obj['Size']\n",
    "                        # print(f\"{obj['Key']} ({obj['Size']} bytes)\")\n",
    "            return files\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "    def download_s3_folder(self, bucket_name, local_dir, prefix=\"\", verbose=0):\n",
    "        \"\"\"\n",
    "        Download all files from an S3 bucket prefix to a local directory.\n",
    "\n",
    "        :param bucket_name: str. Name of the S3 bucket.\n",
    "        :param local_dir: str. Local directory to which files will be downloaded.\n",
    "        :param prefix: str or None. Prefix path of the folder in the bucket. If None, the whole bucket is downloaded.\n",
    "        \"\"\"\n",
    "        if not prefix.endswith(\"/\"):\n",
    "            prefix = prefix + \"/\"\n",
    "        # Ensure local directory exists\n",
    "        if prefix == \"\":\n",
    "            local_dir = os.path.join(local_dir, bucket_name)\n",
    "        else:\n",
    "            local_dir = os.path.join(local_dir, stem(prefix))\n",
    "        if not os.path.exists(local_dir):\n",
    "            os.makedirs(local_dir)\n",
    "\n",
    "        # List objects within the specified prefix\n",
    "        paginator = self.s3_client.get_paginator('list_objects_v2')\n",
    "        for page in paginator.paginate(Bucket=bucket_name, Prefix=prefix):\n",
    "            for obj in page.get('Contents', []):\n",
    "                key = obj['Key']\n",
    "                if not key.endswith('/'):  # skip directories/folders\n",
    "                    # Define file path locally in same structure\n",
    "                    local_file_path = os.path.join(local_dir, key[len(prefix):])\n",
    "                    local_file_dir = os.path.dirname(local_file_path)\n",
    "                    \n",
    "                    # Ensure local file directory exists\n",
    "                    if not os.path.exists(local_file_dir):\n",
    "                        os.makedirs(local_file_dir)\n",
    "\n",
    "                    # Download the file\n",
    "                    self.s3_client.download_file(bucket_name, key, local_file_path)\n",
    "                    if verbose:\n",
    "                        print(f\"Downloaded {key} to {local_file_path}\")\n",
    "\n",
    "    def download_s3_file(self, bucket_name, key, local_dir, metadata=False, verbose=0):\n",
    "        \"\"\"\n",
    "        Download a specific file from an S3 bucket and optionally return its metadata.\n",
    "    \n",
    "        :param bucket_name: str. Name of the S3 bucket.\n",
    "        :param key: str. The key of the file in the S3 bucket.\n",
    "        :param local_dir: str. Local directory to which the file will be downloaded.\n",
    "        :param metadata: bool. If True, return the file's metadata; otherwise, return None.\n",
    "        :param verbose: bool.\n",
    "        :return: dict or None. Returns metadata of the file if metadata is True, otherwise None.\n",
    "        \"\"\"\n",
    "        # Define the local file path\n",
    "        local_file_path = os.path.join(local_dir, os.path.basename(key))\n",
    "\n",
    "        # Ensure the local directory exists\n",
    "        if not os.path.exists(local_dir):\n",
    "            os.makedirs(local_dir)\n",
    "\n",
    "        # Download the file\n",
    "        self.s3_client.download_file(bucket_name, key, local_file_path)\n",
    "        if verbose:\n",
    "            print(f\"Downloaded {key} to {local_file_path}\")\n",
    "\n",
    "        # Optionally retrieve and return metadata\n",
    "        if metadata:\n",
    "            response = self.s3_client.head_object(Bucket=bucket_name, Key=key)\n",
    "            return response  # Return the metadata dictionary\n",
    "        return None\n",
    "\n",
    "    def upload_file_to_s3(self, bucket_name, localfile_path, s3_key, metadata=None):\n",
    "        \"\"\"\n",
    "        Upload a file to an S3 bucket with optional metadata.\n",
    "    \n",
    "        :param bucket_name: str. Name of the S3 bucket.\n",
    "        :param localfile_path: str. Local path to the file to be uploaded.\n",
    "        :param s3_key: str. S3 key (path within the bucket) where the file will be stored with file name included.\n",
    "        :param metadata: dict or None. Optional metadata for the file. Defaults to None.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Setup the file upload options\n",
    "            extra_args = {}\n",
    "            if metadata:\n",
    "                extra_args[\"Metadata\"] = metadata\n",
    "\n",
    "            # Perform the file upload\n",
    "            with open(localfile_path, 'rb') as file_data:\n",
    "                self.s3_client.upload_fileobj(\n",
    "                        Fileobj=file_data,\n",
    "                        Bucket=bucket_name,\n",
    "                        Key=s3_key,\n",
    "                        ExtraArgs=extra_args\n",
    "                       )\n",
    "            if metadata:\n",
    "                print(f\"File uploaded successfully to {bucket_name}/{s3_key} with metadata {metadata}\")\n",
    "            else:\n",
    "                print(f\"File uploaded successfully to {bucket_name}/{s3_key}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to upload file: {e}\")\n",
    "\n",
    "    def inmemory_download_s3(bucket_name, key):\n",
    "        \"\"\"\n",
    "        Downloads a file from an Amazon S3 bucket and loads it directly into a pandas DataFrame. \n",
    "        The function automatically detects the file format based on its extension.\n",
    "\n",
    "        Parameters:\n",
    "        key (str): The S3 object key of the file to download.\n",
    "        bucket (str, optional): The name of the S3 bucket. Defaults to AWS_BUCKET from .env if not provided.\n",
    "        \"\"\"\n",
    "        response = self.s3_client.get_object(Bucket=bucket_name, Key=key)\n",
    "        file_content = response['Body'].read()\n",
    "        return file_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mys3 = S3FileHandler(aws_access_key_id, aws_secret_access_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List all Buckets\n",
    "To lists all the s3 buckets in s3 for given credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['buckettest0011',\n",
       " 'candidate-proctoring',\n",
       " 'sagemaker-ap-south-1-011528263565',\n",
       " 'sagemaker-studio-011528263565-u1h3juay9nd',\n",
       " 'sentiment-classification-fastapi']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mys3.list_s3_buckets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List all file objects\n",
    "List all files in an S3 bucket or within a specific prefix of the given bucket along with the file size.\n",
    "\n",
    ":param bucket_name: str. Name of the S3 bucket.  \n",
    ":param key: str or None. Specific prefix to list files from, defaults to None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test/test/line_profiling_results.txt': 921,\n",
       " 'test/test/outer_function_profile.txt': 2845}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mys3.list_s3_objects(bucket_name=\"buckettest0011\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3 Folder Download\n",
    "Download all files from an S3 bucket prefix to a local directory.\n",
    "\n",
    ":param bucket_name: str. Name of the S3 bucket.  \n",
    ":param local_dir: str. Local directory to which files will be downloaded.  \n",
    ":param prefix: str or None. Prefix path of the folder in the bucket. If None, the whole bucket is downloaded.  \n",
    ":param verbose: bool. Display the download status  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded test/test/line_profiling_results.txt to ./test/line_profiling_results.txt\n",
      "Downloaded test/test/outer_function_profile.txt to ./test/outer_function_profile.txt\n"
     ]
    }
   ],
   "source": [
    "mys3.download_s3_folder(bucket_name=\"buckettest0011\", local_dir='.', prefix=\"test/test\", verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3 File Download\n",
    "Download a specific file from an S3 bucket and optionally return its metadata.\n",
    "\n",
    ":param bucket_name: str. Name of the S3 bucket.  \n",
    ":param key: str. The key of the file in the S3 bucket.  \n",
    ":param local_dir: str. Local directory to which the file will be downloaded.  \n",
    ":param metadata: bool. If True, return the file's metadata; otherwise, return None.  \n",
    ":param verbose: bool.  \n",
    ":return: dict or None. Returns metadata of the file if metadata is True, otherwise None.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '4RT5YGB089R8ER6Y',\n",
       "  'HostId': 'JJCRUZzdH+CUZ5enf4O4r1O2oqr7QFgbmff21q7d8NEgeDDTFTjYl2kH75m3vLp5FaTeA3syDNl8G73FW52w8g==',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'JJCRUZzdH+CUZ5enf4O4r1O2oqr7QFgbmff21q7d8NEgeDDTFTjYl2kH75m3vLp5FaTeA3syDNl8G73FW52w8g==',\n",
       "   'x-amz-request-id': '4RT5YGB089R8ER6Y',\n",
       "   'date': 'Tue, 15 Oct 2024 11:47:02 GMT',\n",
       "   'last-modified': 'Tue, 15 Oct 2024 09:40:40 GMT',\n",
       "   'etag': '\"7c49753bd7d2109ce96bd2568ad8fbef\"',\n",
       "   'x-amz-server-side-encryption': 'AES256',\n",
       "   'x-amz-meta-author': 'XXXXX',\n",
       "   'accept-ranges': 'bytes',\n",
       "   'content-type': 'binary/octet-stream',\n",
       "   'server': 'AmazonS3',\n",
       "   'content-length': '2845'},\n",
       "  'RetryAttempts': 0},\n",
       " 'AcceptRanges': 'bytes',\n",
       " 'LastModified': datetime.datetime(2024, 10, 15, 9, 40, 40, tzinfo=tzutc()),\n",
       " 'ContentLength': 2845,\n",
       " 'ETag': '\"7c49753bd7d2109ce96bd2568ad8fbef\"',\n",
       " 'ContentType': 'binary/octet-stream',\n",
       " 'ServerSideEncryption': 'AES256',\n",
       " 'Metadata': {'author': 'XXXXX'}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mys3.download_s3_file(bucket_name=\"buckettest0011\", key=\"test/test/outer_function_profile.txt\", local_dir=\".\", metadata=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading file from local to s3 with/without metadata\n",
    "Upload a file to an S3 bucket with optional metadata.\n",
    "\n",
    ":param bucket_name: str. Name of the S3 bucket.  \n",
    ":param localfile_path: str. Local path to the file to be uploaded.  \n",
    ":param s3_key: str. S3 key (path within the bucket) where the file will be stored with file name included.  \n",
    ":param metadata: dict or None. Optional metadata for the file. Defaults to None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File uploaded successfully to buckettest0011/test/test/line_profiling_results.txt\n"
     ]
    }
   ],
   "source": [
    "mys3.upload_file_to_s3(bucket_name=\"buckettest0011\",\n",
    "                       localfile_path=\"/home/user/Documents/line_profiling_results.txt\",\n",
    "                       s3_key=\"test/test/line_profiling_results.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File uploaded successfully to buckettest0011/test/test/line_profiling_results.txt with metadata {'author': 'xxxxx'}\n"
     ]
    }
   ],
   "source": [
    "metadata = {\"author\": \"xxxxx\"}\n",
    "mys3.upload_file_to_s3(bucket_name=\"buckettest0011\",\n",
    "                       localfile_path=\"/home/yravi/Documents/line_profiling_results.txt\",\n",
    "                       s3_key=\"test/test/line_profiling_results.txt\",\n",
    "                       metadata=metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets check by downloading the uploaded file if the metadata is present or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded test/test/line_profiling_results.txt to ./line_profiling_results.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '4RTFDPRWMCY0V3KB',\n",
       "  'HostId': '7xhJWRbpiSDCoBpCusjp6HisKzqnC2ofYgK51LHD9lw+NYtromEd0wAipoM3qC8eXfdBmHKnOxSV8jkwz0yi1w==',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': '7xhJWRbpiSDCoBpCusjp6HisKzqnC2ofYgK51LHD9lw+NYtromEd0wAipoM3qC8eXfdBmHKnOxSV8jkwz0yi1w==',\n",
       "   'x-amz-request-id': '4RTFDPRWMCY0V3KB',\n",
       "   'date': 'Tue, 15 Oct 2024 11:47:02 GMT',\n",
       "   'last-modified': 'Tue, 15 Oct 2024 11:47:02 GMT',\n",
       "   'etag': '\"5a627cd11fe9a0ec5877b4a4f0f33a62\"',\n",
       "   'x-amz-server-side-encryption': 'AES256',\n",
       "   'x-amz-meta-author': 'xxxxx',\n",
       "   'accept-ranges': 'bytes',\n",
       "   'content-type': 'binary/octet-stream',\n",
       "   'server': 'AmazonS3',\n",
       "   'content-length': '921'},\n",
       "  'RetryAttempts': 0},\n",
       " 'AcceptRanges': 'bytes',\n",
       " 'LastModified': datetime.datetime(2024, 10, 15, 11, 47, 2, tzinfo=tzutc()),\n",
       " 'ContentLength': 921,\n",
       " 'ETag': '\"5a627cd11fe9a0ec5877b4a4f0f33a62\"',\n",
       " 'ContentType': 'binary/octet-stream',\n",
       " 'ServerSideEncryption': 'AES256',\n",
       " 'Metadata': {'author': 'xxxxx'}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mys3.download_s3_file(bucket_name=\"buckettest0011\", key=\"test/test/line_profiling_results.txt\", local_dir=\".\", metadata=True, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "luminaml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
